# Tu Primer Proyecto

En esta gu√≠a crear√°s tu primer proyecto con ABI-Core paso a paso. Al final tendr√°s un agente funcionando que puedes consultar.

## Lo Que Vas a Construir

Un proyecto simple con:
- ‚úÖ Un agente de IA
- ‚úÖ Modelo de lenguaje (qwen2.5:3b)
- ‚úÖ Interfaz para consultas

**Tiempo estimado**: 10 minutos

## Paso 1: Crear el Proyecto

Abre tu terminal y ejecuta:

```bash
abi-core create project mi-primer-proyecto
```

**¬øQu√© hace este comando?**
- Crea la estructura de directorios
- Configura Docker
- Prepara el entorno

**Salida esperada**:
```
üöÄ Creating ABI project: mi-primer-proyecto
‚úÖ Project structure created
‚úÖ Docker configuration created
‚úÖ Runtime configuration created

üìÅ Project created at: ./mi-primer-proyecto

Next steps:
  cd mi-primer-proyecto
  abi-core provision-models
```

## Paso 2: Navegar al Proyecto

```bash
cd mi-primer-proyecto
```

**Estructura creada**:
```
mi-primer-proyecto/
‚îú‚îÄ‚îÄ agents/              # Aqu√≠ ir√°n tus agentes
‚îú‚îÄ‚îÄ services/            # Servicios de soporte
‚îú‚îÄ‚îÄ compose.yaml         # Configuraci√≥n Docker
‚îú‚îÄ‚îÄ .abi/
‚îÇ   ‚îî‚îÄ‚îÄ runtime.yaml     # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md
```

## Paso 3: Provisionar Modelos

Este paso descarga el modelo de IA que usar√° tu agente:

```bash
abi-core provision-models
```

**¬øQu√© hace este comando?**
1. Inicia el servicio Ollama
2. Descarga `qwen2.5:3b` (~2GB)
3. Descarga modelo de embeddings
4. Actualiza la configuraci√≥n

**Salida esperada**:
```
üöÄ Starting model provisioning...
üì¶ Model serving mode: centralized
üîÑ Starting Ollama service...
‚úÖ Ollama service started

üì• Downloading qwen2.5:3b...
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
‚úÖ qwen2.5:3b downloaded successfully

üì• Downloading nomic-embed-text:v1.5...
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
‚úÖ nomic-embed-text:v1.5 downloaded successfully

‚úÖ Models provisioned successfully
```

**Nota**: La primera vez toma varios minutos dependiendo de tu conexi√≥n.

## Paso 4: Crear Tu Primer Agente

Ahora crea un agente:

```bash
abi-core add agent asistente --description "Mi primer agente de IA"
```

**¬øQu√© hace este comando?**
- Crea el c√≥digo del agente
- Configura el Dockerfile
- Registra el agente en el proyecto

**Salida esperada**:
```
‚úÖ Agent 'asistente' added successfully!
üìÅ Location: agents/asistente
üöÄ Port: 8000
üì¶ Docker service added to compose file
```

**Archivos creados**:
```
agents/asistente/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ agent_asistente.py    # C√≥digo del agente
‚îú‚îÄ‚îÄ main.py               # Punto de entrada
‚îú‚îÄ‚îÄ models.py             # Modelos de datos
‚îú‚îÄ‚îÄ Dockerfile            # Configuraci√≥n Docker
‚îî‚îÄ‚îÄ requirements.txt      # Dependencias
```

## Paso 5: Iniciar el Sistema

Inicia todos los servicios:

```bash
abi-core run
```

O con Docker Compose directamente:

```bash
docker-compose up -d
```

**¬øQu√© se inicia?**
- Servicio Ollama (modelos de IA)
- Tu agente asistente

**Verificar que est√° funcionando**:
```bash
docker-compose ps
```

Deber√≠as ver:
```
NAME                          STATUS    PORTS
mi-primer-proyecto-ollama     Up        0.0.0.0:11434->11434/tcp
asistente-agent               Up        0.0.0.0:8000->8000/tcp
```

## Paso 6: Probar Tu Agente

### Opci√≥n 1: Con curl

```bash
curl -X POST http://localhost:8000/stream \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hola, ¬øc√≥mo est√°s?",
    "context_id": "test-001",
    "task_id": "task-001"
  }'
```

**Respuesta esperada**:
```json
{
  "content": "¬°Hola! Estoy bien, gracias por preguntar. Soy tu asistente de IA. ¬øEn qu√© puedo ayudarte hoy?",
  "response_type": "text",
  "is_task_completed": true
}
```

### Opci√≥n 2: Con Python

Crea un archivo `test_agent.py`:

```python
import requests

response = requests.post(
    "http://localhost:8000/stream",
    json={
        "query": "¬øQu√© es la inteligencia artificial?",
        "context_id": "test-001",
        "task_id": "task-001"
    }
)

print(response.json())
```

Ejecuta:
```bash
python test_agent.py
```

### Opci√≥n 3: Navegador

Abre tu navegador y ve a:
```
http://localhost:8000/docs
```

Ver√°s la interfaz Swagger donde puedes probar el agente interactivamente.

## Paso 7: Ver Logs

Para ver qu√© est√° haciendo tu agente:

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver solo logs del agente
docker-compose logs -f asistente-agent

# Ver solo logs de Ollama
docker-compose logs -f mi-primer-proyecto-ollama
```

## Paso 8: Detener el Sistema

Cuando termines:

```bash
# Detener servicios
docker-compose down

# Detener y eliminar vol√∫menes (modelos)
docker-compose down -v
```

## Entendiendo Tu Proyecto

### Archivo: `agents/asistente/agent_asistente.py`

Este es el c√≥digo principal de tu agente:

```python
from abi_core.agent.agent import AbiAgent
from abi_core.common.utils import abi_logging

class AsistenteAgent(AbiAgent):
    """Mi primer agente de IA"""
    
    def __init__(self):
        super().__init__(
            agent_name='asistente',
            description='Mi primer agente de IA',
            content_types=['text/plain']
        )
        self.setup_llm()
    
    def setup_llm(self):
        """Configura el modelo de lenguaje"""
        from langchain_ollama import ChatOllama
        import os
        
        model = os.getenv('MODEL_NAME', 'qwen2.5:3b')
        ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        
        self.llm = ChatOllama(
            model=model,
            base_url=ollama_host,
            temperature=0.7
        )
    
    def process(self, enriched_input):
        """Procesa la consulta del usuario"""
        query = enriched_input['query']
        
        abi_logging(f"Procesando: {query}")
        
        # Usa el LLM para generar respuesta
        response = self.llm.invoke(query)
        
        return {
            'result': response.content,
            'query': query
        }
    
    async def stream(self, query: str, context_id: str, task_id: str):
        """Responde a consultas en streaming"""
        
        # Procesa con enriquecimiento sem√°ntico
        result = self.handle_input(query)
        
        # Retorna respuesta
        yield {
            'content': result['result'],
            'response_type': 'text',
            'is_task_completed': True,
            'require_user_input': False
        }
```

### Archivo: `.abi/runtime.yaml`

Configuraci√≥n del proyecto:

```yaml
project:
  name: mi-primer-proyecto
  domain: general
  model_serving: centralized

agents:
  asistente:
    name: Asistente
    description: Mi primer agente de IA
    model: qwen2.5:3b
    port: 8000
    path: agents/asistente

models:
  llm:
    name: qwen2.5:3b
    provisioned: true
  embedding:
    name: nomic-embed-text:v1.5
    provisioned: true
```

### Archivo: `compose.yaml`

Configuraci√≥n de Docker:

```yaml
services:
  mi-primer-proyecto-ollama:
    image: ollama/ollama:latest
    container_name: mi-primer-proyecto-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - mi-primer-proyecto-network
  
  asistente-agent:
    build: ./agents/asistente
    container_name: asistente-agent
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=qwen2.5:3b
      - OLLAMA_HOST=http://mi-primer-proyecto-ollama:11434
    depends_on:
      - mi-primer-proyecto-ollama
    networks:
      - mi-primer-proyecto-network

volumes:
  ollama_data:

networks:
  mi-primer-proyecto-network:
    driver: bridge
```

## Personalizar Tu Agente

### Cambiar el Modelo

Edita `.abi/runtime.yaml`:

```yaml
agents:
  asistente:
    model: llama3.2:3b  # Cambiar a otro modelo
```

Descarga el nuevo modelo:
```bash
docker exec mi-primer-proyecto-ollama ollama pull llama3.2:3b
```

Reinicia:
```bash
docker-compose restart asistente-agent
```

### Cambiar el Puerto

Edita `compose.yaml`:

```yaml
asistente-agent:
  ports:
    - "9000:8000"  # Cambiar puerto externo
```

Reinicia:
```bash
docker-compose up -d
```

### Modificar el Comportamiento

Edita `agents/asistente/agent_asistente.py`:

```python
def setup_llm(self):
    self.llm = ChatOllama(
        model='qwen2.5:3b',
        base_url=ollama_host,
        temperature=0.1  # M√°s determinista
        # temperature=0.9  # M√°s creativo
    )
```

Reconstruye:
```bash
docker-compose up -d --build asistente-agent
```

## Soluci√≥n de Problemas

### Error: "Port already in use"

**Causa**: El puerto 8000 ya est√° en uso.

**Soluci√≥n**: Cambia el puerto en `compose.yaml` o det√©n el servicio que lo usa.

### Error: "Model not found"

**Causa**: El modelo no se descarg√≥ correctamente.

**Soluci√≥n**:
```bash
docker exec mi-primer-proyecto-ollama ollama pull qwen2.5:3b
```

### Error: "Connection refused"

**Causa**: Ollama no est√° funcionando.

**Soluci√≥n**:
```bash
docker-compose restart mi-primer-proyecto-ollama
docker-compose logs mi-primer-proyecto-ollama
```

### El agente responde muy lento

**Causa**: El modelo es grande para tu hardware.

**Soluci√≥n**: Usa un modelo m√°s peque√±o:
```bash
docker exec mi-primer-proyecto-ollama ollama pull phi3:mini
```

Actualiza la configuraci√≥n para usar `phi3:mini`.

## Pr√≥ximos Pasos

¬°Felicidades! Ya tienes tu primer proyecto funcionando. Ahora puedes:

1. [Crear un chatbot m√°s complejo](../single-agent/02-simple-chatbot.md)
2. [Agregar herramientas a tu agente](../single-agent/03-agents-with-tools.md)
3. [Agregar memoria conversacional](../single-agent/04-agents-with-memory.md)

## Resumen

En esta gu√≠a aprendiste a:

- ‚úÖ Crear un proyecto con `abi-core create project`
- ‚úÖ Provisionar modelos con `abi-core provision-models`
- ‚úÖ Crear un agente con `abi-core add agent`
- ‚úÖ Iniciar el sistema con `abi-core run`
- ‚úÖ Probar tu agente con HTTP
- ‚úÖ Ver logs y depurar
- ‚úÖ Personalizar configuraci√≥n

---

**Creado por [Jos√© Luis Mart√≠nez](https://github.com/Joselo-zn)** | jl.mrtz@gmail.com
